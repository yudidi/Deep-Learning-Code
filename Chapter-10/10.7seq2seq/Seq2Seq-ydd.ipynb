{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "好的，如果你只是想修改训练数据以展示从英文到中文的翻译原理，而并非追求实际的翻译效果，那么你可以将原始的训练数据中的英文目标词汇替换为中文。这里使用的中文词汇会被转换为拼音，因为当前模型是基于字符级别的处理，而且简单的RNN结构可能无法很好地处理中文字符。\n",
    "\n",
    "这里是一个简化的示例，其中的中文拼音是硬编码的：\n",
    "\n",
    "```python\n",
    "# 原始数据：英文输入和英文输出\n",
    "# seq_data = [['man', 'women'], ['black', 'white'], ...]\n",
    "\n",
    "# 修改后的数据：英文输入和中文输出（使用拼音）\n",
    "seq_data = [['man', 'nuren'], ['black', 'bai'], ['king', 'nuwang'], ['girl', 'nanhai'], ['up', 'xia'], ['high', 'di']]\n",
    "\n",
    "# 注意：为了简洁起见，这里的中文拼音不包括声调，并且中文翻译可能并不精确，只是为了演示目的。\n",
    "```\n",
    "\n",
    "由于我们使用了拼音代替中文字符，因此不需要对字符集进行任何修改，因为拼音可以用ASCII字符表示。但是，如果你想使用实际的中文字符，你需要确保字符集包括所有必要的中文字符，并且修改`char_arr`和`num_dic`以适应新的字符集。\n",
    "\n",
    "这种方法的缺点是，它不会教会模型如何处理实际的中文字符。它只是一个简化的表示，用于说明从一个语言到另一个语言的基本翻译过程。在真实的应用场景中，你需要使用专门的工具和方法来处理中文文本，例如使用中文分词器，以及在模型中采用更高级的NLP技术。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "seq_data = [['man', 'nuren'], ['black', 'bai'], ['king', 'nuwang'], ['girl', 'nanhai'], ['up', 'xia'], ['high', 'di']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/rnn.py:70: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost = 0.009075\n",
      "Epoch: 2000 cost = 0.002335\n",
      "Epoch: 3000 cost = 0.000975\n",
      "Epoch: 4000 cost = 0.000482\n",
      "Epoch: 5000 cost = 0.000258\n",
      "test\n",
      "man -> nuren\n",
      "mans -> nuren\n",
      "king -> nuwang\n",
      "black -> bai\n",
      "upp -> xia\n"
     ]
    }
   ],
   "source": [
    "# code by Tae Hwan Jung @graykode\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time steps\n",
    "\n",
    "def make_batch():\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
    "\n",
    "        input = [num_dic[n] for n in seq[0]]\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        output_batch.append(np.eye(n_class)[output])\n",
    "        target_batch.append(target) # not one-hot\n",
    "\n",
    "    # make tensor\n",
    "    return torch.FloatTensor(input_batch), torch.FloatTensor(output_batch), torch.LongTensor(target_batch)\n",
    "\n",
    "# make test batch\n",
    "def make_testbatch(input_word):\n",
    "    input_batch, output_batch = [], []\n",
    "\n",
    "    input_w = input_word + 'P' * (n_step - len(input_word))\n",
    "    input = [num_dic[n] for n in input_w]\n",
    "    output = [num_dic[n] for n in 'S' + 'P' * n_step]\n",
    "\n",
    "    input_batch = np.eye(n_class)[input]\n",
    "    output_batch = np.eye(n_class)[output]\n",
    "\n",
    "    return torch.FloatTensor(input_batch).unsqueeze(0), torch.FloatTensor(output_batch).unsqueeze(0)\n",
    "\n",
    "# Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        self.enc_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.dec_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    def forward(self, enc_input, enc_hidden, dec_input):\n",
    "        enc_input = enc_input.transpose(0, 1) # enc_input: [max_len(=n_step, time step), batch_size, n_class]\n",
    "        dec_input = dec_input.transpose(0, 1) # dec_input: [max_len(=n_step, time step), batch_size, n_class]\n",
    "\n",
    "        # enc_states : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "        _, enc_states = self.enc_cell(enc_input, enc_hidden)\n",
    "        # outputs : [max_len+1(=6), batch_size, num_directions(=1) * n_hidden(=128)]\n",
    "        outputs, _ = self.dec_cell(dec_input, enc_states)\n",
    "\n",
    "        model = self.fc(outputs) # model : [max_len+1(=6), batch_size, n_class]\n",
    "        return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # n_step = 5\n",
    "    n_step = 7 # ydd 参考gpt查找的bug\n",
    "    n_hidden = 128\n",
    "\n",
    "    char_arr = [c for c in 'SEPabcdefghijklmnopqrstuvwxyz']\n",
    "    num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "    # seq_data = [['man', 'women'], ['black', 'white'], ['king', 'queen'], ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "    n_class = len(num_dic)\n",
    "    batch_size = len(seq_data)\n",
    "\n",
    "    model = Seq2Seq()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    input_batch, output_batch, target_batch = make_batch()\n",
    "\n",
    "    for epoch in range(5000):\n",
    "        # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
    "        hidden = torch.zeros(1, batch_size, n_hidden)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # input_batch : [batch_size, max_len(=n_step, time step), n_class]\n",
    "        # output_batch : [batch_size, max_len+1(=n_step, time step) (becase of 'S' or 'E'), n_class]\n",
    "        # target_batch : [batch_size, max_len+1(=n_step, time step)], not one-hot\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        # output : [max_len+1, batch_size, n_class]\n",
    "        output = output.transpose(0, 1) # [batch_size, max_len+1(=6), n_class]\n",
    "        loss = 0\n",
    "        for i in range(0, len(target_batch)):\n",
    "            # output[i] : [max_len+1, n_class, target_batch[i] : max_len+1]\n",
    "            loss += criterion(output[i], target_batch[i])\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test\n",
    "    def translate(word):\n",
    "        input_batch, output_batch = make_testbatch(word)\n",
    "\n",
    "        # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
    "        hidden = torch.zeros(1, 1, n_hidden)\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        # output : [max_len+1(=6), batch_size(=1), n_class]\n",
    "\n",
    "        predict = output.data.max(2, keepdim=True)[1] # select n_class dimension\n",
    "        decoded = [char_arr[i] for i in predict]\n",
    "        end = decoded.index('E')\n",
    "        translated = ''.join(decoded[:end])\n",
    "\n",
    "        return translated.replace('P', '')\n",
    "\n",
    "    print('test')\n",
    "    print('man ->', translate('man'))\n",
    "    print('mans ->', translate('mans'))\n",
    "    print('king ->', translate('king'))\n",
    "    print('black ->', translate('black'))\n",
    "    print('upp ->', translate('upp'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blackup -> bai\n",
      "upphigh -> xia\n"
     ]
    }
   ],
   "source": [
    "    print('blackup ->', translate('blackup'))\n",
    "    print('upphigh ->', translate('upphigh'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blackup -> bai\n",
      "upphigh -> xia\n"
     ]
    }
   ],
   "source": [
    "    print('blackup ->', translate('blackup'))\n",
    "    print('upphigh ->', translate('upphigh'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "    def translate(word):\n",
    "        input_batch, output_batch = make_testbatch(word)\n",
    "\n",
    "        # make hidden shape [num_layers * num_directions, batch_size, n_hidden]\n",
    "        hidden = torch.zeros(1, 1, n_hidden)\n",
    "        output = model(input_batch, hidden, output_batch)\n",
    "        # output : [max_len+1(=6), batch_size(=1), n_class]\n",
    "\n",
    "        predict = output.data.max(2, keepdim=True)[1] # select n_class dimension\n",
    "        decoded = [char_arr[i] for i in predict]\n",
    "        end = decoded.index('E')\n",
    "        translated = ''.join(decoded[:end])\n",
    "\n",
    "        return translated.replace('P', '')\n",
    "```\n",
    "这段代码定义了一个名为 `translate` 的函数，它接受一个单词作为输入，并返回该单词的翻译。以下是这个函数的主要部分的简要解释：\n",
    "\n",
    "1. `make_testbatch(word)`: 这个函数看起来是用来将输入的单词转换成模型可处理的格式。它返回两个批次变量：`input_batch` 和 `output_batch`。这些批次可能包含输入单词的某种形式的编码，例如一个独热编码向量或其他类型的数值表示。\n",
    "\n",
    "2. `hidden = torch.zeros(1, 1, n_hidden)`: 这一行创建了一个用于模型的初始隐藏状态的张量。该张量的维度是 `[num_layers * num_directions, batch_size, n_hidden]`，这里设定为了 `(1, 1, n_hidden)`，其中 `n_hidden` 是隐藏层的大小。\n",
    "\n",
    "3. `output = model(input_batch, hidden, output_batch)`: 这里模型被调用，使用输入批次、隐藏状态和输出批次作为参数。模型的输出被赋值给 `output` 变量。这个输出是一个三维张量，其中包含了对每个时间步长的每个可能类别的预测。\n",
    "\n",
    "4. `predict = output.data.max(2, keepdim=True)[1]`: 这行代码从 `output` 张量中选择预测值最大的索引，这通常代表了预测概率最高的类别。参数 `2` 表示我们在第三个维度（即类别维度）中选择最大值。\n",
    "\n",
    "5. `decoded = [char_arr[i] for i in predict]`: 这行代码将预测的索引转换回字符。`char_arr` 是一个列表或数组，其索引对应于可能的输出字符。\n",
    "\n",
    "6. `end = decoded.index('E')`: 这里，我们在 `decoded` 列表中查找字符 `'E'`，这通常用于表示序列的结束。\n",
    "\n",
    "7. `translated = ''.join(decoded[:end])`: 这行将解码的字符（不包括找到的结束字符 `'E'`）连接成一个字符串，这就是翻译的单词。\n",
    "\n",
    "8. `return translated.replace('P', '')`: 最后，返回的翻译结果中将所有的 `'P'` 字符移除。在序列到序列的模型中，`'P'` 通常用来填充短序列以匹配最长序列的长度。这里移除它是为了清理输出结果，让翻译的单词不包含任何填充字符。\n",
    "\n",
    "总之，这个 `translate` 函数将输入单词通过一个序列到序列的模型进行翻译，并处理输出结果，去除序列结束标记和填充字符，最终提供一个干净的翻译字符串。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}